{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from os import walk\n",
    "from keras.preprocessing import sequence\n",
    "from scipy.io import wavfile\n",
    "import glob\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import keras\n",
    "pool=ThreadPool(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/tf/logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "#%tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wavefile\n",
    "def readwav(file:str):\n",
    "    filepath = Path(file).absolute()\n",
    "    samplerate, data = wavfile.read((filepath))\n",
    "    # print(f\"samplerate = {samplerate}\")\n",
    "    return data,samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTiming(data:np.ndarray,samplerate:int):\n",
    "    length = data.shape[0] / samplerate\n",
    "    return np.arange(0,length,1/samplerate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSignal(data:np.ndarray,t:np.ndarray,plot:bool=True,length=None,filter=True):\n",
    "    ## normalize input\n",
    "    sig = data/np.amax(data)\n",
    "    norm_heart = data/np.amax(data)\n",
    "    sos = signal.butter(1, [.2,195], 'bp', fs=1000, output='sos')\n",
    "    filtered_heart = signal.sosfilt(sos, sig)\n",
    "    ## Removing noise\n",
    "    noise_heart = signal.signaltools.wiener(filtered_heart,300)\n",
    "    noise_heart = filtered_heart\n",
    "    if(not filter):\n",
    "        noise_heart = norm_heart\n",
    "    if length:\n",
    "        resampled,resampledt = signal.resample(noise_heart,33075,t=t)\n",
    "    if plot:\n",
    "        if length:\n",
    "            # _, (ax1, ax2,ax3,ax4) = plt.subplots(4, 1, sharex=True)\n",
    "            # ax4.plot(resampledt, resampled)\n",
    "            # ax4.set_title('After Resampling')\n",
    "            # ax4.set_xlabel('Time [seconds]')\n",
    "            _, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "            ax1.plot(t, sig)\n",
    "            ax1.set_title('Original Heart Rate Signal')\n",
    "            ax2.plot(resampledt, resampled)\n",
    "            ax2.set_title('After Resampling')\n",
    "            ax2.set_xlabel('Time [seconds]')\n",
    "        \n",
    "        #     _, (ax1, ax2,ax3) = plt.subplots(3, 1, sharex=True)\n",
    "        # ax1.plot(t, sig)\n",
    "        # ax1.set_title('Original Heart Rate Signal')\n",
    "        # ax2.plot(t, norm_heart)\n",
    "        # ax2.set_title('After Bandpass filter')\n",
    "        # ax3.plot(t, noise_heart)\n",
    "        # ax3.set_title('After Noise Filter')\n",
    "        # ax3.set_xlabel('Time [seconds]')\n",
    "\n",
    "        # plt.tight_layout()\n",
    "    plt.show()\n",
    "    return (resampled,resampledt) if length else (noise_heart,t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "def generateSignal(file:str,plot:bool=False,loglevel:str=None):\n",
    "    data,samplerate = readwav(file)\n",
    "    length = data.shape[0] / samplerate\n",
    "    lengths.append(length)\n",
    "    t = getTiming(data,samplerate)\n",
    "\n",
    "    sig,t = filterSignal(data,t,plot,length)\n",
    "    return t,sig,samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFolder = \"./heartbeats/classifications\"\n",
    "trainingpath = Path(trainingFolder)\n",
    "paths = [Path(dir[0]) for dir in walk(trainingpath)][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "# test_data = []\n",
    "plot = False\n",
    "def processFiles(indexedWave,classification,trainIndex):\n",
    "    wav,index = indexedWave\n",
    "\n",
    "    t,d,_ = generateSignal(wav,plot)\n",
    "    # if(index < trainIndex):\n",
    "    train_data.append([d,t,classification])\n",
    "    # else:\n",
    "        # test_data.append([d,t,classification])\n",
    "\n",
    "\n",
    "def get_training_data(path:Path):   \n",
    "    classification = path.name\n",
    "    wavList = glob.glob(str(path.joinpath(\"*.wav\")))\n",
    "    trainIndex=int(math.ceil(len(wavList)*.8)) # use 80% of data for training\n",
    "    pool.map(lambda x: processFiles(x,classification,trainIndex),zip(wavList,range(0,len(wavList))) )\n",
    "    # for wav in zip(wavList,range(0,len(wavList))):\n",
    "    #     processFiles(wav,classification,trainIndex)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data \n",
    "#Loading data from this many files is intensive, speeding up w/ multithreading\n",
    "\n",
    "for path in paths:\n",
    "    get_training_data(path)\n",
    "#train_data=np.array(train_data)\n",
    "#test_data=np.array(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig,time,classification = zip(*train_data)\n",
    "getMaxLength = lambda list: max([len(item) for item in list])\n",
    "max_length = getMaxLength(sig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_data,columns=[\"signal\",\"time\",\"classification\"])\n",
    "# print(df[[\"signal\",\"time\"]].values)\n",
    "xs=sequence.pad_sequences(df.signal.values,maxlen=max_length,dtype=\"float64\")\n",
    "xt=sequence.pad_sequences(df.time.values,maxlen=max_length,dtype=\"float64\")\n",
    "\n",
    "# y = df.iloc\n",
    "# sequence.pad_sequences(df[[\"signal\",\"time\"]].values,maxlen=max_length,dtype=\"float64\")\n",
    "# print(df.iloc[2])\n",
    "df = pd.get_dummies(df,columns=[\"classification\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 33075, 2)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.iloc[:,2:].values\n",
    "x = np.dstack((xs,xt))\n",
    "# x = xs\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 15, 2205, 2)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature extraction\n",
    "n_steps, n_length = 15, 2205\n",
    "x = x.reshape((x.shape[0], n_steps,1, n_length, 2))\n",
    "# x = x.reshape((x.shape[0], n_steps, n_length, 2))\n",
    "\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 15, 1, 2205, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "# y=tf.keras.utils.to_categorical(np.array(y),num_classes=5)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d_19 (ConvLSTM2D)  (None, 1, 2203, 32)      13184     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 1, 2203, 32)       0         \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 70496)             0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 100)               7049700   \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 5)                 505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,063,389\n",
      "Trainable params: 7,063,389\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = Sequential()\n",
    "    model.add(layers.ConvLSTM2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ConvLSTM2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(.2))\n",
    "\n",
    "    model.add(layers.ConvLSTM2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ConvLSTM2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(.2))\n",
    "\n",
    "    model.add(layers.ConvLSTM2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ConvLSTM2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(.2))\n",
    "    # model.add(Conv1D(filters=16, kernel_size=4, activation='relu'))\n",
    "\n",
    "    # model.add(ConvLSTM2D(filters=16, kernel_size=(1,3), activation='relu'))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(5, activation='softmax'))\n",
    "    model.build(np.shape(x_train))\n",
    "    print(model.summary())\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=5. Full shape received: (158, 15, 1, 2202, 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28971/2386029319.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    438\u001b[0m               'method accepts an `inputs` argument.')\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m           raise ValueError('You cannot build your model by calling `build` '\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\u001b[0m\u001b[1;32m    214\u001b[0m                          \u001b[0;34m'is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                          \u001b[0;34mf'expected ndim={spec.ndim}, found ndim={ndim}. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=5. Full shape received: (158, 15, 1, 2202, 16)"
     ]
    }
   ],
   "source": [
    "# # define model\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "#     model = Sequential()\n",
    "#     # model.add(ConvLSTM2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "#     model.add(Conv1D(filters=16, kernel_size=4, activation='relu'))\n",
    "#     model.add(LSTM(100))\n",
    "\n",
    "#     # # model.add(ConvLSTM2D(filters=16, kernel_size=(1,3), activation='relu'))\n",
    "\n",
    "#     # model.add(Dropout(0.5))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(5, activation='softmax'))\n",
    "#     model.build(np.shape(x_train))\n",
    "#     print(model.summary())\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 07:42:23.269561: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_110311\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\023FlatMapDataset:3387\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2/2 [==============================] - 3s 134ms/step - loss: 56.6852 - accuracy: 0.3291\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 455.1083 - accuracy: 0.2215\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 471.9447 - accuracy: 0.1519\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 269.0144 - accuracy: 0.1519\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 65.9166 - accuracy: 0.2215\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 38.3713 - accuracy: 0.1076\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 43.8853 - accuracy: 0.1962\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 19.9428 - accuracy: 0.1962\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 18.3397 - accuracy: 0.1203\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 24.6772 - accuracy: 0.2785\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 26.6622 - accuracy: 0.2975\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 24.1149 - accuracy: 0.3418\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 19.6884 - accuracy: 0.3354\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 14.2433 - accuracy: 0.3291\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 8.5050 - accuracy: 0.3165\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 9.7679 - accuracy: 0.2342\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 6.1576 - accuracy: 0.3608\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 7.5382 - accuracy: 0.2215\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 5.2506 - accuracy: 0.2215\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 1.9441 - accuracy: 0.2722\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 1.5210 - accuracy: 0.3797\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 1.5173 - accuracy: 0.3797\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 1.5276 - accuracy: 0.3797\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 1.5346 - accuracy: 0.3734\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 1.5345 - accuracy: 0.3797\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 1.5238 - accuracy: 0.3797\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 1.5106 - accuracy: 0.3861\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 1.5276 - accuracy: 0.3165\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 1.5273 - accuracy: 0.3734\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 1.5569 - accuracy: 0.3544\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 1.5717 - accuracy: 0.3481\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 1.5904 - accuracy: 0.3354\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 1.6073 - accuracy: 0.3228\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 1.6071 - accuracy: 0.3228\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 1.6069 - accuracy: 0.3228\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 1.6067 - accuracy: 0.3228\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 1.6065 - accuracy: 0.3228\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 1.6063 - accuracy: 0.3228\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 1.6061 - accuracy: 0.3228\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 1.6059 - accuracy: 0.3228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x153ebc6f02b0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=40, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 07:36:31.734283: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_107507\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\023FlatMapDataset:3312\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x153c2d7aff70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[3.262042999267578, 0.4444444477558136]\n",
      "Accuracy: 1.853% (+/-1.409)\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(scores)\n",
    "m, s = np.mean(scores), np.std(scores)\n",
    "print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 07:36:39.326590: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_108498\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\023FlatMapDataset:3338\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
